SELECT
  round(sum(total_amount)), z."Borough" 
FROM
  ny_taxi.public.yellow_taxi_trips ytt , "zone" z 
WHERE
  DATE(lpep_pickup_datetime) = '2019-09-18' and z."Borough" != 'Unknown'
  group by z."Borough" 
  
  
SELECT
  ytt."DOLocationID",  ytt.tip_amount  ,ytt."PULocationID" 
FROM
  ny_taxi.public.yellow_taxi_trips ytt , "zone" z 
where
ytt."PULocationID" = z."LocationID" and z."Zone" = 'Astoria'  



select z."Zone" 
from yellow_taxi_trips ytt , "zone" z 
where ytt."DOLocationID" = z."LocationID" 
and ytt."DOLocationID" = 132



select z."Zone" 
from yellow_taxi_trips ytt , "zone" z 
where ytt."DOLocationID" = z."LocationID" 
and ytt."DOLocationID" = 132



hw2 

data_loader


import io
import pandas as pd
import requests

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@data_loader
def load_data_from_api(*args, **kwargs):
    import pandas as pd
    from io import BytesIO
    import requests
    from datetime import datetime


    base_url = "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/"

    df_final = pd.DataFrame()

    for year in range(2020, 2020 + 1):
        for month in range(10, 12 + 1):  # Loop through all months
            print(year, month)
            filename = f"green_tripdata_{year:04d}-{month:02d}.csv.gz"
            df_temp = pd.read_csv(base_url+filename, sep=',', compression='gzip', parse_dates=['lpep_pickup_datetime', 'lpep_dropoff_datetime'])
            df_final = pd.concat([df_final, df_temp], ignore_index=True)

    print(df_final.info())
    print(df_final.shape)
    return df_final




@test
def test_output(output, *args) -> None:
    """
    Template code for testing the output of the block.
    """
    assert output is not None, 'The output is undefined'




transformer


if 'transformer' not in globals():
    from mage_ai.data_preparation.decorators import transformer
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test

import pandas as pd

@transformer
def transform(data, *args, **kwargs):

    print(data.shape)
    print(data.info())
    print(data[(data['passenger_count'] > 0) & (data['trip_distance'] >  0)].info())
    print('valeurs uniques' ,data['VendorID'].nunique())
    print('valeurs uniques' ,data['VendorID'].unique())

    data = data[(data['passenger_count'] != 0) & (data['trip_distance'] != 0)]
    data['lpep_pickup_datetime'] = pd.to_datetime(data['lpep_pickup_datetime'])
    data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date

    import re

    def camel_to_snake(name):
        snake_case = re.sub('([a-z0-9])([A-Z])', r'\1_\2', name).lower()
        return snake_case

    data.columns = [camel_to_snake(c) for c in data.columns]

    print(data.info())

    return data


@test
def test_output(output, *args) -> None:
    # Assertion for vendor_id
    assert output['vendor_id'].isin(output['vendor_id'].unique()).all(), "Not all vendor_id values are valid."

    # Assertion for passenger_count
    assert output['passenger_count'].min() > 0, "There are rows with non-positive passenger_count."

    # Assertion for trip_distance
    assert output['trip_distance'].min() > 0, "There are rows with non-positive trip_distance."

    # Optional: Print confirmation message if assertions pass
    print("All assertions passed! Data seems valid.")
    assert output is not None, 'The output is undefined'





data exporter 


from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.postgres import Postgres
from pandas import DataFrame
from os import path

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data_to_postgres(df: DataFrame, **kwargs) -> None:
    """
    Template for exporting data to a PostgreSQL database.
    Specify your configuration settings in 'io_config.yaml'.

    Docs: https://docs.mage.ai/design/data-loading#postgresql
    """
    schema_name = 'mage'  # Specify the name of the schema to export data to
    table_name = 'green_taxi'  # Specify the name of the table to export data to
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'env'

    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
        loader.export(
            df,
            schema_name,
            table_name,
            index=False,  # Specifies whether to include index in exported table
            if_exists='replace',  # Specify resolution policy if table name already exists
        )



data exporter to gcs


import pyarrow as pa
import pyarrow.parquet as pq
import os


if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "/home/src/personal-gcp.json/XXXXXXXXXXXXXXXXXXX.json"
bucket_name = 'magic-zoomcamp'
object_key = 'nyc_taxi_data.parquet'
project_id = 'XXXXXXXXXXXXXXXXXX'
table_name = 'nyc_taxi_data'
root_path = f"{bucket_name}/{table_name}"

@data_exporter
def export_data(data, *args, **kwargs):
    table = pa.Table.from_pandas(data)
    gcs = pa.fs.GcsFileSystem()

    pq.write_to_dataset(
        table,
        root_path=root_path,
        partition_cols=['lpep_pickup_date'],
        filesystem = gcs

    )



create table before read parquet url https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page


import os
import psycopg2

def load_postgres_config(filename):
    """Loads environment variables from a specified file."""
    with open(filename) as f:
        for line in f:
            key, value = line.strip().split('=')
            os.environ[key] = value

def create_table(cursor, table_name, schema, query):
    """Creates a table in the specified schema."""
    try:
        cursor.execute(f"CREATE SCHEMA IF NOT EXISTS {schema}")
        cursor.execute(f"CREATE TABLE {schema}.{table_name} {query}")
    except Exception as e:
        print(f"Error creating table: {e}")

def load_data(connection, cursor, schema, table_name, data_directory):
    """Loads data from Parquet files into the specified table."""
    try:
        df = pd.read_parquet(data_directory)
        # Convert data types if needed (add logic here)
        cursor.executemany(f"INSERT INTO {schema}.{table_name} VALUES (...)", df.to_records(index=False))
        connection.commit()
    except Exception as e:
        print(f"Error loading data: {e}")

# Load credentials from .postgres file
load_postgres_config(".postgres")

# Extract Postgres connection details from environment variables
dbname = os.environ["POSTGRES_DBNAME"]
user = os.environ["POSTGRES_USER"]
password = os.environ["POSTGRES_PASSWORD"]
host = 'localhost'
port = int(os.environ["POSTGRES_PORT"])
schema = os.environ["POSTGRES_SCHEMA"]  # Optional schema for table creation

# Replace with your desired table name and schema
table_name = "green_taxi_trips_2022"
query = """
(
    vendor_id BIGINT,
    passenger_count INT,
    trip_distance FLOAT,
    ratecode_id INT,
    store_and_fwd_flag VARCHAR(50),
    pulocation_id INT,
    dolocation_id INT,
    payment_type INT,
    fare_amount FLOAT,
    extra FLOAT,
    mta_tax FLOAT,
    tip_amount FLOAT,
    tolls_amount FLOAT,
    improvement_surcharge FLOAT,
    total_amount FLOAT
)
"""

try:
    # Connect to Postgres
    connection = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)
    cursor = connection.cursor()

    # Create the table (adjust schema if needed)
    create_table(cursor, table_name, schema, query)

    # ... (continue with data loading as planned)

except Exception as e:
    print(f"Error connecting or creating table: {e}")
finally:
    if connection:
        connection.close()

